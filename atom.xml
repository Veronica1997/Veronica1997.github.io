<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Veronica1997</title>
  
  <subtitle>傲不可长，欲不可纵，志不可满，乐不可极</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-07-14T09:03:58.847Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Veronica_ry</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>PyTorch实战入门</title>
    <link href="http://yoursite.com/2019/07/14/PyTorch%E5%AE%9E%E6%88%98%E5%85%A5%E9%97%A8/"/>
    <id>http://yoursite.com/2019/07/14/PyTorch实战入门/</id>
    <published>2019-07-14T09:11:43.563Z</published>
    <updated>2019-07-14T09:03:58.847Z</updated>
    
    <content type="html"><![CDATA[<h4 id="最常用命名规范小结"><a href="#最常用命名规范小结" class="headerlink" title="最常用命名规范小结"></a>最常用命名规范小结</h4><p><img src="https://img-blog.csdnimg.cn/20190711191310391.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zOTc2NDU2OQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h4 id="常用的程序库"><a href="#常用的程序库" class="headerlink" title="常用的程序库"></a>常用的程序库</h4><p><img src="https://img-blog.csdnimg.cn/20190711191422220.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zOTc2NDU2OQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h4 id="文件组织"><a href="#文件组织" class="headerlink" title="文件组织"></a>文件组织</h4><p>不要将所有的层和模型放在同一个文件中。最好的做法是将最终的网络分离到独立的文件（networks.py）中，并将层、损失函数以及各种操作保存在各自的文件中（layers.py，losses.py，ops.py）。最终得到的模型（由一个或多个网络组成）应该用该模型的名称命名（例如，yolov3.py，DCGAN.py），且引用各个模块。<strong>主程序、单独的训练和测试脚本应该只需要导入带有模型名字的 Python 文件。</strong> </p><h4 id="PyTorch-开发风格与技巧"><a href="#PyTorch-开发风格与技巧" class="headerlink" title="PyTorch 开发风格与技巧"></a>PyTorch 开发风格与技巧</h4><p>最好将网络分解为更小的可复用的片段。一个 nn.Module 网络包含各种操作或其它构建模块。损失函数也是包含在 nn.Module 内，因此它们可以被直接整合到网络中。<br>继承 nn.Module 的类必须拥有一个「forward」方法，它实现了各个层或操作的前向传导。<br>一个 nn.module 可以通过「self.net(input)」处理输入数据。在这里直接使用了对象的「call()」方法将输入数据传递给模块。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">output = self.net(input)</span><br></pre></td></tr></table></figure></p><h4 id="Tensor的认识"><a href="#Tensor的认识" class="headerlink" title="Tensor的认识"></a>Tensor的认识</h4><p>Tensor可以认为是一个高维数组，和Numpy相似，但Tensor可以用GPU加速；<br>Tensor与Numpy之间的转换，互操作比较容易且快速，Tensor不支持的操作，可以先转换为Numpy数组处理，之后再转回Tensor。</p><h4 id="Variable的了解"><a href="#Variable的了解" class="headerlink" title="Variable的了解"></a>Variable的了解</h4><p>Variable是Pytorch中autograd自动微分模块的核心，它封装了Tensor,支持几乎所有的tensor操作。主要包含如下3个属性：</p><ol><li>data: 保存Variable所包含的Tensor。</li><li>grad: 保存data对应的梯度，grad也是一个Variable，而不是一个Tensor，和data的形状一样。</li><li>grad_fn: 指向一个Function对象，这个Function用来反向传播计算输入的梯度。<h4 id="nn-Module模块详解"><a href="#nn-Module模块详解" class="headerlink" title="nn.Module模块详解"></a>nn.Module模块详解</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">torch.nn是专门为神经网络设计的模块化接口。nn构建于autograd之上，可以用来定义和运行神经网络。</span></span><br><span class="line"><span class="string">nn.Module是nn中十分重要的类,包含网络各层的定义及forward方法。</span></span><br><span class="line"><span class="string">在定义网络的时候，如果层内有Variable,那么用nn定义，反之，则用nn.functional定义。</span></span><br><span class="line"><span class="string">定义自已的网络：</span></span><br><span class="line"><span class="string">    需要继承nn.Module类，并实现forward方法。</span></span><br><span class="line"><span class="string">    一般把网络中具有可学习参数的层放在构造函数__init__()中，</span></span><br><span class="line"><span class="string">    不具有可学习参数的层(如ReLU)可放在构造函数中，也可不放在构造函数中(而在forward中使用nn.functional来代替)</span></span><br><span class="line"><span class="string">    只要在nn.Module的子类中定义了forward函数，backward函数就会被自动实现(利用Autograd)。</span></span><br><span class="line"><span class="string">    在forward函数中可以使用任何Variable支持的函数，毕竟在整个pytorch构建的图中，是Variable在流动。还可以使用if,for,print,log等python语法.</span></span><br><span class="line"><span class="string">    注：Pytorch基于nn.Module构建的模型中，只支持mini-batch的Variable输入方式，</span></span><br><span class="line"><span class="string">    比如，只有一张输入图片，也需要变成 N x C x H x W 的形式：</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    input_image = torch.FloatTensor(1, 28, 28)</span></span><br><span class="line"><span class="string">    input_image = Variable(input_image)</span></span><br><span class="line"><span class="string">    input_image = input_image.unsqueeze(0)   # 1 x 1 x 28 x 28</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LeNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># nn.Module的子类函数必须在构造函数中执行父类的构造函数</span></span><br><span class="line">        <span class="comment">#self指的是类实例对象本身(注意：不是类本身)</span></span><br><span class="line">        super(LeNet, self).__init__()   <span class="comment"># 等价与nn.Module.__init__()</span></span><br><span class="line"> <span class="comment"># super() 函数是用于调用父类(超类)的一个方法</span></span><br><span class="line">        <span class="comment"># nn.Conv2d返回的是一个Conv2d class的一个对象，该类中包含forward函数的实现</span></span><br><span class="line">        <span class="comment"># 当调用self.conv1(input)的时候，就会调用该类的forward函数</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, (<span class="number">5</span>, <span class="number">5</span>))   <span class="comment"># output (N, C_&#123;out&#125;, H_&#123;out&#125;, W_&#123;out&#125;)`</span></span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, (<span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">256</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">        <span class="comment"># nn.Linear(inputSize, outputSize)输入和输出节点数</span></span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv1(x)), (<span class="number">2</span>, <span class="number">2</span>))  <span class="comment"># F.max_pool2d的返回值是一个Variable</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv2(x)), (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">        <span class="comment"># torch.nn.functional.max_pool2d(input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False, return_indices=False)</span></span><br><span class="line">        x = x.view(x.size()[<span class="number">0</span>], <span class="number">-1</span>)</span><br><span class="line">        <span class="comment"># x = x.view(x.size()[0], -1)将多维度的tensor展平为一维，forward()函数中，input首先经过卷积层，此时的输出x是包含batchsize维度为4的tensor，即(batchsize，channels，x，y)，x.size(0)指batchsize的值。</span></span><br><span class="line">        <span class="comment"># x = x.view(x.size(0), -1)简化x = x.view(batchsize, -1)。</span></span><br><span class="line">        <span class="comment"># view()函数的功能根reshape类似，用来转换size大小。x = x.view(batchsize, -1)中batchsize指转换后有几行，而-1指在不告诉函数有多少列的情况下，根据原tensor数据和batchsize自动分配列数。</span></span><br><span class="line">        <span class="comment"># -1是自适应的意思，x.size(0)是batch size，比如原来的数据一共12个，batch size为2，就会view成2*6，batch size为4，就会就会view成4*3。</span></span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = F.relu(self.fc3(x))</span><br><span class="line">        <span class="comment"># torch.nn.functional.relu(input, inplace=False)</span></span><br><span class="line">        <span class="comment"># 返回值也是一个Variable对象</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">output_name_and_params</span><span class="params">(net)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> name, parameters <span class="keyword">in</span> net.named_parameters():</span><br><span class="line">        print(<span class="string">'name: &#123;&#125;, param: &#123;&#125;'</span>.format(name, parameters))</span><br><span class="line"> <span class="comment"># 查看可训练的参数，包括从父类中继承的</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    net = LeNet()</span><br><span class="line">    print(<span class="string">'net: &#123;&#125;'</span>.format(net))   <span class="comment"># str.format()，它增强了字符串格式化</span></span><br><span class="line">    <span class="comment"># "&#123;1&#125; &#123;0&#125; &#123;1&#125;".format("hello", "world")  # 设置指定位置</span></span><br><span class="line">    <span class="comment"># 上面语句输出结果：'world hello world'</span></span><br><span class="line">    params = net.parameters()   <span class="comment"># generator object</span></span><br><span class="line">    print(<span class="string">'params: &#123;&#125;'</span>.format(params))</span><br><span class="line">    output_name_and_params(net)</span><br><span class="line"> </span><br><span class="line">    input_image = torch.FloatTensor(<span class="number">10</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 和tensorflow不一样，pytorch中模型的输入是一个Variable，而且是Variable在图中流动，不是Tensor。</span></span><br><span class="line">    <span class="comment"># 这可以从forward中每一步的执行结果可以看出</span></span><br><span class="line">    input_image = Variable(input_image)</span><br><span class="line">    <span class="comment"># Variable是篮子，而tensor是鸡蛋，鸡蛋应该放在篮子里才能方便拿走（定义variable时一个参数就是tensor）</span></span><br><span class="line">    output = net(input_image)</span><br><span class="line">    print(<span class="string">'output: &#123;&#125;'</span>.format(output))</span><br><span class="line">    print(<span class="string">'output.size: &#123;&#125;'</span>.format(output.size()))</span><br></pre></td></tr></table></figure></li></ol><h4 id="PyTorch环境下的一个简单网络"><a href="#PyTorch环境下的一个简单网络" class="headerlink" title="PyTorch环境下的一个简单网络"></a>PyTorch环境下的一个简单网络</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ConvBlock</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(ConvBlock, self).__init__()</span><br><span class="line">        block = [nn.Conv2d(...)]</span><br><span class="line">        block += [nn.ReLU()]</span><br><span class="line">        block += [nn.BatchNorm2d(...)]</span><br><span class="line">        self.block = nn.Sequential(*block)</span><br><span class="line"><span class="comment"># 复用了简单的循环构建模块（如卷积块 ConvBlocks），它们由相同的循环模式（卷积、激活函数、归一化）组成，并装入独立的 nn.Module 中。</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.block(x)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleNetwork</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_resnet_blocks=<span class="number">6</span>)</span>:</span></span><br><span class="line">        super(SimpleNetwork, self).__init__()</span><br><span class="line">        <span class="comment"># here we add the individual layers</span></span><br><span class="line">        layers = [ConvBlock(...)]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_resnet_blocks):</span><br><span class="line">            layers += [ResBlock(...)]</span><br><span class="line">        self.net = nn.Sequential(*layers)</span><br><span class="line"><span class="comment"># 构建了一个所需要层的列表，并最终使用「nn.Sequential()」将所有层级组合到了一个模型中。我们在 list 对象前使用「*」操作来展开它。</span></span><br><span class="line"><span class="comment"># 必须使用「*」，若不使用，则会报错：list is not a Module subclass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.net(x)</span><br><span class="line"><span class="comment"># 在前向传导过程中，我们直接使用输入数据运行模型</span></span><br></pre></td></tr></table></figure><h4 id="PyTorch-环境下的简单残差网络"><a href="#PyTorch-环境下的简单残差网络" class="headerlink" title="PyTorch 环境下的简单残差网络"></a>PyTorch 环境下的简单残差网络</h4><p>ResNet 模块的跳跃连接直接在前向传导过程中实现，PyTorch 允许在前向传导过程中进行动态操作。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResnetBlock</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, dim, padding_type, norm_layer, use_dropout, use_bias)</span>:</span></span><br><span class="line">        super(ResnetBlock, self).__init__()</span><br><span class="line">        self.conv_block = self.build_conv_block(...)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build_conv_block</span><span class="params">(self, ...)</span>:</span></span><br><span class="line">        conv_block = []</span><br><span class="line"></span><br><span class="line">        conv_block += [nn.Conv2d(...),</span><br><span class="line">                       norm_layer(...),</span><br><span class="line">                       nn.ReLU()]</span><br><span class="line">        <span class="keyword">if</span> use_dropout:</span><br><span class="line">            conv_block += [nn.Dropout(...)]</span><br><span class="line"></span><br><span class="line">        conv_block += [nn.Conv2d(...),</span><br><span class="line">                       norm_layer(...)]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*conv_block)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        out = x + self.conv_block(x)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure></p><h4 id="Pytorch中数据加载—Dataset类和DataLoader类"><a href="#Pytorch中数据加载—Dataset类和DataLoader类" class="headerlink" title="Pytorch中数据加载—Dataset类和DataLoader类"></a>Pytorch中数据加载—Dataset类和DataLoader类</h4><h4 id="PyTorch-环境下的带多个输出的网络"><a href="#PyTorch-环境下的带多个输出的网络" class="headerlink" title="PyTorch 环境下的带多个输出的网络"></a>PyTorch 环境下的带多个输出的网络</h4><p>对于有多个输出的网络（例如使用一个预训练好的 VGG 网络构建感知损失），我们使用以下模式:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Vgg19</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, requires_grad=False)</span>:</span></span><br><span class="line">    super(Vgg19, self).__init__()</span><br><span class="line">    vgg_pretrained_features = models.vgg19(pretrained=<span class="keyword">True</span>).features</span><br><span class="line">    <span class="comment"># 使用由「torchvision」包提供的预训练模型</span></span><br><span class="line">    self.slice1 = torch.nn.Sequential()</span><br><span class="line">    self.slice2 = torch.nn.Sequential()</span><br><span class="line">    self.slice3 = torch.nn.Sequential()</span><br><span class="line">    <span class="comment"># 将一个网络切分成三个模块，每个模块由预训练模型中的层组成</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">7</span>):</span><br><span class="line">        self.slice1.add_module(str(x), vgg_pretrained_features[x])</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">7</span>, <span class="number">21</span>):</span><br><span class="line">        self.slice2.add_module(str(x), vgg_pretrained_features[x])</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">21</span>, <span class="number">30</span>):</span><br><span class="line">        self.slice3.add_module(str(x), vgg_pretrained_features[x])</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> requires_grad:</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> self.parameters():</span><br><span class="line">            param.requires_grad = <span class="keyword">False</span></span><br><span class="line">            <span class="comment"># 通过设置「requires_grad = False」来固定网络权重</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">    h_relu1 = self.slice1(x)</span><br><span class="line">    h_relu2 = self.slice2(h_relu1)        </span><br><span class="line">    h_relu3 = self.slice3(h_relu2)        </span><br><span class="line">    out = [h_relu1, h_relu2, h_relu3]</span><br><span class="line">    <span class="keyword">return</span> out</span><br><span class="line">    <span class="comment"># 返回一个带有三个模块输出的 list</span></span><br></pre></td></tr></table></figure></p><h4 id="自定义损失函数"><a href="#自定义损失函数" class="headerlink" title="自定义损失函数"></a>自定义损失函数</h4><p>即使 PyTorch 已经具有了大量标准损失函数，你有时也可能需要创建自己的损失函数。为了做到这一点，你需要创建一个独立的「losses.py」文件，并且通过扩展「nn.Module」创建你的自定义损失函数：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomLoss</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(CustomLoss,self).__init__()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,x,y)</span>:</span></span><br><span class="line">        loss = torch.mean((x - y)**<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure></p><h4 id="Dataset类和DataLoader类"><a href="#Dataset类和DataLoader类" class="headerlink" title="Dataset类和DataLoader类"></a>Dataset类和DataLoader类</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"><span class="keyword">import</span> matplotlib <span class="keyword">as</span> mpl</span><br><span class="line">mpl.use(<span class="string">'tkagg'</span>)   <span class="comment"># 调试：agg;  运行： tkagg</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="comment"># pandas 是基于NumPy 的一种工具,该工具是为了解决数据分析任务而创建的</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"> </span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">torch.utils.data.Dataset 是一个表示数据集的抽象类.</span></span><br><span class="line"><span class="string">你自己的数据集一般应该继承``Dataset``, 并且重写下面的方法:</span></span><br><span class="line"><span class="string">    1. __len__ 使用``len(dataset)`` 可以返回数据集的大小</span></span><br><span class="line"><span class="string">    2. __getitem__ 支持索引, 以便于使用 dataset[i] 可以 获取第i个样本(0索引)</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">torch.utils.data中的DataLoader提供为Dataset类对象提供了:</span></span><br><span class="line"><span class="string">    1.批量读取数据</span></span><br><span class="line"><span class="string">    2.打乱数据顺序</span></span><br><span class="line"><span class="string">    3.使用multiprocessing并行加载数据</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    DataLoader中的一个参数collate_fn：可以使用它来指定如何精确地读取一批样本，</span></span><br><span class="line"><span class="string">     merges a list of samples to form a mini-batch.</span></span><br><span class="line"><span class="string">    然而，默认情况下collate_fn在大部分情况下都表现很好</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms, utils</span><br><span class="line"><span class="keyword">from</span> skimage <span class="keyword">import</span> io, transform</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">just_see_face_dataset</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    摸一下数据</span></span><br><span class="line"><span class="string">    :return: </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    landmarks_frame = pd.read_csv(<span class="string">'./faces/face_landmarks.csv'</span>)</span><br><span class="line">    n = <span class="number">65</span></span><br><span class="line">    img_name = landmarks_frame.iloc[n, <span class="number">0</span>]</span><br><span class="line">    landmarks = landmarks_frame.iloc[n, <span class="number">1</span>:].as_matrix()    <span class="comment"># as_matrix()</span></span><br><span class="line">    landmarks = landmarks.astype(<span class="string">'float'</span>).reshape(<span class="number">-1</span>, <span class="number">2</span>)</span><br><span class="line">    print(<span class="string">'img_name: &#123;&#125;'</span>.format(img_name))</span><br><span class="line">    print(<span class="string">'landmarks shape: &#123;&#125;'</span>.format(landmarks.shape))</span><br><span class="line">    print(<span class="string">'first 4 landmarks: &#123;&#125;'</span>.format(landmarks[:<span class="number">4</span>]))</span><br><span class="line"> </span><br><span class="line">    plt.figure()</span><br><span class="line">    show_landmarks(io.imread(os.path.join(<span class="string">'faces'</span>, img_name)), landmarks)</span><br><span class="line">    plt.show()</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_landmarks</span><span class="params">(image, landmarks)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    显示一张图片和它对应的标记点</span></span><br><span class="line"><span class="string">    :param image:</span></span><br><span class="line"><span class="string">    :param landmarks:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    plt.imshow(image)</span><br><span class="line">    plt.scatter(landmarks[:, <span class="number">0</span>], landmarks[:, <span class="number">1</span>], s=<span class="number">10</span>, marker=<span class="string">'.'</span>, c=<span class="string">'r'</span>)</span><br><span class="line">    plt.pause(<span class="number">0.001</span>)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FaceLandmarksDataset</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, csv_file, root_dir, transform=None)</span>:</span></span><br><span class="line">        self.landmarks_frame = pd.read_csv(csv_file)</span><br><span class="line">        self.root_dir = root_dir</span><br><span class="line">        self.transform = transform</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        继承 Dataset 类后,必须重写的一个方法</span></span><br><span class="line"><span class="string">        返回数据集的大小</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> len(self.landmarks_frame)</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, idx)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        继承 Dataset 类后,必须重写的一个方法</span></span><br><span class="line"><span class="string">        返回第 idx 个图像及相关信息</span></span><br><span class="line"><span class="string">        :param idx:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        img_name = os.path.join(self.root_dir, self.landmarks_frame.iloc[idx, <span class="number">0</span>])</span><br><span class="line">        image = io.imread(img_name)</span><br><span class="line">        landmarks = self.landmarks_frame.iloc[idx, <span class="number">1</span>:].as_matrix()</span><br><span class="line">        landmarks = landmarks.astype(<span class="string">'float'</span>).reshape(<span class="number">-1</span>, <span class="number">2</span>)</span><br><span class="line">        sample = &#123;<span class="string">'image'</span>: image, <span class="string">'landmarks'</span>: landmarks&#125;</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">if</span> self.transform:</span><br><span class="line">            sample = self.transform(sample)</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">return</span> sample</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">t_dataset</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    测试 FaceLandmarksDataset 类的使用</span></span><br><span class="line"><span class="string">    :return: </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 实列化 FaceLandmarksDataset 类</span></span><br><span class="line">    face_dataset = FaceLandmarksDataset(csv_file=<span class="string">'./faces/face_landmarks.csv'</span>, root_dir=<span class="string">'./faces'</span>)</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    length_dataset = len(face_dataset)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(length_dataset):</span><br><span class="line">        <span class="comment"># 注: Dataset 类对象可以直接索引[i]访问</span></span><br><span class="line">        sample = face_dataset[i]</span><br><span class="line">        print(i, sample[<span class="string">'image'</span>].shape, sample[<span class="string">'landmarks'</span>].shape)</span><br><span class="line"> </span><br><span class="line">        ax = plt.subplot(<span class="number">1</span>, <span class="number">4</span>, i + <span class="number">1</span>)</span><br><span class="line">        plt.tight_layout()</span><br><span class="line">        ax.set_title(<span class="string">'sample #&#123;&#125;'</span>.format(i))</span><br><span class="line">        ax.axis(<span class="string">'off'</span>)</span><br><span class="line">        show_landmarks(sample[<span class="string">'image'</span>], sample[<span class="string">'landmarks'</span>])</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">3</span>:</span><br><span class="line">            plt.show()</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="string">"""Transform操作"""</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Rescale</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""按照给定尺寸更改一个图像的尺寸</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        output_size (tuple or int): 要求输出的尺寸.  如果是个元组类型, 输出</span></span><br><span class="line"><span class="string">        和output_size匹配. 如果时int类型,图片的短边和output_size匹配, 图片的</span></span><br><span class="line"><span class="string">        长宽比保持不变.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, output_size)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> isinstance(output_size, (int, tuple))</span><br><span class="line">        self.output_size = output_size</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, sample)</span>:</span></span><br><span class="line">        image, landmarks = sample[<span class="string">'image'</span>], sample[<span class="string">'landmarks'</span>]</span><br><span class="line"> </span><br><span class="line">        h, w = image.shape[:<span class="number">2</span>]</span><br><span class="line">        <span class="keyword">if</span> isinstance(self.output_size, int):</span><br><span class="line">            <span class="keyword">if</span> h &gt; w:</span><br><span class="line">                new_h, new_w = self.output_size * h / w, self.output_size</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                new_h, new_w = self.output_size, self.output_size * w / h</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            new_h, new_w = self.output_size</span><br><span class="line"> </span><br><span class="line">        new_h, new_w = int(new_h), int(new_w)</span><br><span class="line"> </span><br><span class="line">        img = transform.resize(image, (new_h, new_w))</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># 对于标记点, h和w需要交换位置, 因为对于图像, x和y分别时第1维和第0维</span></span><br><span class="line">        landmarks = landmarks * [new_w / w, new_h / h]</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># 返回值实际上也是一个sample</span></span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">'image'</span>: img, <span class="string">'landmarks'</span>: landmarks&#125;</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RandomCrop</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""随机裁剪图片</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        output_size (tuple or int): 期望输出的尺寸, 如果时int类型, 裁切成正方形.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, output_size)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> isinstance(output_size, (int, tuple))</span><br><span class="line">        <span class="keyword">if</span> isinstance(output_size, int):</span><br><span class="line">            self.output_size = (output_size, output_size)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">assert</span> len(output_size) == <span class="number">2</span></span><br><span class="line">            self.output_size = output_size</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, sample)</span>:</span></span><br><span class="line">        image, landmarks = sample[<span class="string">'image'</span>], sample[<span class="string">'landmarks'</span>]</span><br><span class="line"> </span><br><span class="line">        h, w = image.shape[:<span class="number">2</span>]</span><br><span class="line">        new_h, new_w = self.output_size</span><br><span class="line"> </span><br><span class="line">        top = np.random.randint(<span class="number">0</span>, h - new_h)</span><br><span class="line">        left = np.random.randint(<span class="number">0</span>, w - new_w)</span><br><span class="line"> </span><br><span class="line">        image = image[top: top + new_h,</span><br><span class="line">                      left: left + new_w]</span><br><span class="line"> </span><br><span class="line">        landmarks = landmarks - [left, top]</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># 返回值实际上也是一个sample</span></span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">'image'</span>: image, <span class="string">'landmarks'</span>: landmarks&#125;</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ToTensor</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    将 ndarray 的样本转化为 Tensor 的样本</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, sample)</span>:</span></span><br><span class="line">        image, landmarks = sample[<span class="string">'image'</span>], sample[<span class="string">'landmarks'</span>]</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># 交换轴，因为 numpy 图片：H x W x C, torch输入图片要求： C x H x W</span></span><br><span class="line">        image = image.transpose((<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">'image'</span>: torch.from_numpy(image), <span class="string">'landmarks'</span>: torch.from_numpy(landmarks)&#125;</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">use_transoform</span><span class="params">(one_sample)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    演示如何使用 transform: 把几种 transform 组合在一起</span></span><br><span class="line"><span class="string">    :return: </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># transforms.Compose 只是将这两种tranform组合在一起，按顺序对sample进行处理</span></span><br><span class="line">    composed = transforms.Compose([Rescale(<span class="number">256</span>), RandomCrop(<span class="number">224</span>)])</span><br><span class="line">    transfromed_sample = composed(one_sample)</span><br><span class="line">    plt.figure()</span><br><span class="line">    show_landmarks(transfromed_sample[<span class="string">'image'</span>], transfromed_sample[<span class="string">'landmarks'</span>])</span><br><span class="line">    plt.show()</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">union_all_knowledge</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    迭代整个数据集：</span></span><br><span class="line"><span class="string">        每次迭代数据，都会1.从文件中读取图像    2.对所读取的图像应用上述变换transform。 从而对数据集进行增强操作</span></span><br><span class="line"><span class="string">    :return: </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    transformed_dataset = FaceLandmarksDataset(csv_file=<span class="string">'./faces/face_landmarks.csv'</span>, root_dir=<span class="string">'./faces'</span>,</span><br><span class="line">                                               transform=transforms.Compose([</span><br><span class="line">                                                   Rescale(<span class="number">256</span>),</span><br><span class="line">                                                   RandomCrop(<span class="number">225</span>),</span><br><span class="line">                                                   ToTensor()]))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(transformed_dataset)):</span><br><span class="line">        sample = transformed_dataset[i]</span><br><span class="line">        print(i, sample[<span class="string">'image'</span>].size(), sample[<span class="string">'landmarks'</span>].size())</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">3</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">t_dataloader</span><span class="params">()</span>:</span></span><br><span class="line">    transformed_dataset = FaceLandmarksDataset(csv_file=<span class="string">'./faces/face_landmarks.csv'</span>, root_dir=<span class="string">'./faces'</span>,</span><br><span class="line">                                               transform=transforms.Compose([</span><br><span class="line">                                                   Rescale(<span class="number">256</span>),</span><br><span class="line">                                                   RandomCrop(<span class="number">225</span>),</span><br><span class="line">                                                   ToTensor()]))</span><br><span class="line">    dataloader = DataLoader(transformed_dataset, batch_size=<span class="number">4</span>, shuffle=<span class="keyword">True</span>, num_workers=<span class="number">2</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># 对dataloader对象进行迭代，读取数据</span></span><br><span class="line">    <span class="keyword">for</span> i_batch, sample_batched <span class="keyword">in</span> enumerate(dataloader):</span><br><span class="line">        image_batch, landmarks_batch = sample_batched[<span class="string">'image'</span>], sample_batched[<span class="string">'landmarks'</span>]</span><br><span class="line">        print(<span class="string">'i_batch: &#123;&#125;, image_batch.size(): &#123;&#125;, landmarks_batch.size(): &#123;&#125;'</span>.format(</span><br><span class="line">            i_batch, image_batch.size(), landmarks_batch.size()))</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment"># just_see_face_dataset()</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment"># t_dataset()</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment"># face_dataset = FaceLandmarksDataset(csv_file='./faces/face_landmarks.csv', root_dir='./faces')</span></span><br><span class="line">    <span class="comment"># one_sample = face_dataset[0]</span></span><br><span class="line">    <span class="comment"># use_transoform(one_sample)</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment"># union_all_knowledge()</span></span><br><span class="line"> </span><br><span class="line">    t_dataloader()</span><br></pre></td></tr></table></figure><h4 id="训练模型的最佳代码结构"><a href="#训练模型的最佳代码结构" class="headerlink" title="训练模型的最佳代码结构"></a>训练模型的最佳代码结构</h4><p>对于训练的最佳代码结构，我们需要使用以下两种模式：</p><ul><li>使用 prefetch_generator 中的 BackgroundGenerator 来加载下一个批量数据</li><li>使用 tqdm 监控训练过程，并展示计算效率，这能帮助我们找到数据加载流程中的瓶颈<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># import statements</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="comment"># set flags / seeds</span></span><br><span class="line">torch.backends.cudnn.benchmark = <span class="keyword">True</span></span><br><span class="line"><span class="comment"># torch.backends.cudnn.benchmark = True 在程序刚开始加这条语句可以提升一点训练速度，没什么额外开销。我一般都会加</span></span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line">torch.cuda.manual_seed(<span class="number">1</span>)</span><br><span class="line">...</span><br><span class="line"><span class="comment"># 每次运行代码时设置相同的seed，则每次生成的随机数也相同，如果不设置seed，则每次生成的随机数都会不一样。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Start with main code</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment"># argparse for additional flags for experiment</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    argparse是python用于解析命令行参数和选项的标准模块</span></span><br><span class="line"><span class="string">    argparse使用步骤：</span></span><br><span class="line"><span class="string">    1：import argparse  # 首先导入该模块</span></span><br><span class="line"><span class="string">    2：parser = argparse.ArgumentParser() # 然后创建一个解析对象</span></span><br><span class="line"><span class="string">    3：parser.add_argument() # 向该对象中添加你要关注的命令行参数和选项，然后每一个add_argument方法对应一个你要关注的参数或选项</span></span><br><span class="line"><span class="string">    4：parser.parse_args() # 最后调用parse_args()方法进行解析，解析成功之后即可使用</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    parser = argparse.ArgumentParser(description=<span class="string">"Train a network for ..."</span>)</span><br><span class="line">    ...</span><br><span class="line">    opt = parser.parse_args() </span><br><span class="line"></span><br><span class="line">    <span class="comment"># add code for datasets (we always use train and validation/ test set)</span></span><br><span class="line">    data_transforms = transforms.Compose([</span><br><span class="line">        transforms.Resize((opt.img_size, opt.img_size)),</span><br><span class="line">        transforms.RandomHorizontalFlip(),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">    train_dataset = datasets.ImageFolder(</span><br><span class="line">        root=os.path.join(opt.path_to_data, <span class="string">"train"</span>),</span><br><span class="line">        transform=data_transforms)</span><br><span class="line">    train_data_loader = data.DataLoader(train_dataset, ...)</span><br><span class="line"></span><br><span class="line">    test_dataset = datasets.ImageFolder(</span><br><span class="line">        root=os.path.join(opt.path_to_data, <span class="string">"test"</span>),</span><br><span class="line">        transform=data_transforms)</span><br><span class="line">    test_data_loader = data.DataLoader(test_dataset ...)</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="comment"># instantiate network (which has been imported from *networks.py*)</span></span><br><span class="line">    net = MyNetwork(...)</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="comment"># create losses (criterion in pytorch)</span></span><br><span class="line">    criterion_L1 = torch.nn.L1Loss()</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="comment"># if running on GPU and we want to use cuda move model there</span></span><br><span class="line">    use_cuda = torch.cuda.is_available()</span><br><span class="line">    <span class="keyword">if</span> use_cuda:</span><br><span class="line">        net = net.cuda()</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="comment"># create optimizers</span></span><br><span class="line">    optim = torch.optim.Adam(net.parameters(), lr=opt.lr)</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="comment"># load checkpoint if needed/ wanted</span></span><br><span class="line">    start_n_iter = <span class="number">0</span></span><br><span class="line">    start_epoch = <span class="number">0</span></span><br><span class="line">    <span class="keyword">if</span> opt.resume:</span><br><span class="line">        ckpt = load_checkpoint(opt.path_to_checkpoint) <span class="comment"># custom method for loading last checkpoint</span></span><br><span class="line">        net.load_state_dict(ckpt[<span class="string">'net'</span>])</span><br><span class="line">        start_epoch = ckpt[<span class="string">'epoch'</span>]</span><br><span class="line">        start_n_iter = ckpt[<span class="string">'n_iter'</span>]</span><br><span class="line">        optim.load_state_dict(ckpt[<span class="string">'optim'</span>])</span><br><span class="line">        print(<span class="string">"last checkpoint restored"</span>)</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="comment"># if we want to run experiment on multiple GPUs we move the models there</span></span><br><span class="line">    net = torch.nn.DataParallel(net)</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="comment"># typically we use tensorboardX to keep track of experiments</span></span><br><span class="line">    writer = SummaryWriter(...)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># now we start the main loop</span></span><br><span class="line">    n_iter = start_n_iter</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(start_epoch, opt.epochs):</span><br><span class="line">        <span class="comment"># set models to train mode</span></span><br><span class="line">        net.train()</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">        <span class="comment"># use prefetch_generator and tqdm for iterating through data</span></span><br><span class="line">        pbar = tqdm(enumerate(BackgroundGenerator(train_data_loader, ...)),</span><br><span class="line">                    total=len(train_data_loader))</span><br><span class="line">        start_time = time.time()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># for loop going through dataset</span></span><br><span class="line">        <span class="keyword">for</span> i, data <span class="keyword">in</span> pbar:</span><br><span class="line">            <span class="comment"># data preparation</span></span><br><span class="line">            img, label = data</span><br><span class="line">            <span class="keyword">if</span> use_cuda:</span><br><span class="line">                img = img.cuda()</span><br><span class="line">                label = label.cuda()</span><br><span class="line">            ...</span><br><span class="line"></span><br><span class="line">            <span class="comment"># It's very good practice to keep track of preparation time and computation time using tqdm to find any issues in your dataloader</span></span><br><span class="line">            prepare_time = start_time-time.time()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># forward and backward pass</span></span><br><span class="line">            optim.zero_grad()</span><br><span class="line">            ...</span><br><span class="line">            loss.backward()</span><br><span class="line">            optim.step()</span><br><span class="line">            ...</span><br><span class="line"></span><br><span class="line">            <span class="comment"># udpate tensorboardX</span></span><br><span class="line">            writer.add_scalar(..., n_iter)</span><br><span class="line">            ...</span><br><span class="line"></span><br><span class="line">            <span class="comment"># compute computation time and *compute_efficiency*</span></span><br><span class="line">            process_time = start_time-time.time()-prepare_time</span><br><span class="line">            pbar.set_description(<span class="string">"Compute efficiency: &#123;:.2f&#125;, epoch: &#123;&#125;/&#123;&#125;:"</span>.format(</span><br><span class="line">                process_time/(process_time+prepare_time), epoch, opt.epochs))</span><br><span class="line">            start_time = time.time()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># maybe do a test pass every x epochs</span></span><br><span class="line">        <span class="keyword">if</span> epoch % x == x<span class="number">-1</span>:</span><br><span class="line">            <span class="comment"># bring models to evaluation mode</span></span><br><span class="line">            net.eval()</span><br><span class="line">            ...</span><br><span class="line">            <span class="comment">#do some tests</span></span><br><span class="line">            pbar = tqdm(enumerate(BackgroundGenerator(test_data_loader, ...)),</span><br><span class="line">                    total=len(test_data_loader)) </span><br><span class="line">            <span class="keyword">for</span> i, data <span class="keyword">in</span> pbar:</span><br><span class="line">                ...</span><br><span class="line"></span><br><span class="line">            <span class="comment"># save checkpoint if needed</span></span><br><span class="line">            ...</span><br></pre></td></tr></table></figure></li></ul><h4 id="使用PyTorch注意事项"><a href="#使用PyTorch注意事项" class="headerlink" title="使用PyTorch注意事项"></a>使用PyTorch注意事项</h4><ol><li>在「nn.Module」的「forward」方法中避免使用 Numpy 代码。Numpy 是在 CPU 上运行的，它比 torch 的代码运行得要慢一些。由于 torch 的开发思路与 numpy 相似，所以大多数 Numpy 中的函数已经在 PyTorch 中得到了支持。</li><li>将「DataLoader」从主程序的代码中分离。载入数据的工作流程应该独立于你的主训练程序代码。PyTorch 使用「background」进程更加高效地载入数据，而不会干扰到主训练进程。</li><li><p>使用命令行参数。使用命令行参数设置代码执行时使用的参数（batch 的大小、学习率等）非常方便。一个简单的实验参数跟踪方法，即直接把从「parse_args」接收到的字典（dict 数据）打印出来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># saves arguments to config.txt file</span></span><br><span class="line">opt = parser.parse_args()<span class="keyword">with</span> open(<span class="string">"config.txt"</span>, <span class="string">"w"</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(opt.__str__())</span><br></pre></td></tr></table></figure></li><li><p>如果可能的话，请使用「Use .detach()」从计算图中释放张量。为了实现自动微分，PyTorch 会跟踪所有涉及张量的操作。请使用「.detach()」来防止记录不必要的操作。</p></li><li>使用「.item()」打印出标量张量。你可以直接打印变量。然而，我们建议你使用「variable.detach()」或「variable.item()」。在早期版本的 PyTorch（&lt; 0.4）中，你必须使用「.data」访问变量中的张量值。</li><li>使用「call」方法代替「nn.Module」中的「forward」方法。这两种方式并不完全相同.原文链接：<a href="https://github.com/IgorSusmelj/pytorch-styleguide" target="_blank" rel="noopener">https://github.com/IgorSusmelj/pytorch-styleguide</a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">output = self.net.forward(input)</span><br><span class="line"><span class="comment"># they are not equal!</span></span><br><span class="line">output = self.net(input)</span><br></pre></td></tr></table></figure></li></ol><h4 id="参考文献："><a href="#参考文献：" class="headerlink" title="参考文献："></a>参考文献：</h4><p>[1][<a href="https://mp.weixin.qq.com/s/6OxnjoaR2SQINKk9U_OrtQ" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/6OxnjoaR2SQINKk9U_OrtQ</a> ]<br>[2][<a href="https://blog.csdn.net/u012609509/article/details/81203436" target="_blank" rel="noopener">https://blog.csdn.net/u012609509/article/details/81203436</a> ]<br>[3][<a href="https://blog.csdn.net/u012609509/article/details/81203308" target="_blank" rel="noopener">https://blog.csdn.net/u012609509/article/details/81203308</a> ]<br>[4][<a href="https://blog.csdn.net/u012609509/article/details/81203376" target="_blank" rel="noopener">https://blog.csdn.net/u012609509/article/details/81203376</a> ]<br>[5][<a href="https://blog.csdn.net/u012609509/article/details/81264687" target="_blank" rel="noopener">https://blog.csdn.net/u012609509/article/details/81264687</a> ]<br>[6][<a href="https://blog.csdn.net/GZHermit/article/details/78730856" target="_blank" rel="noopener">https://blog.csdn.net/GZHermit/article/details/78730856</a> ]</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;最常用命名规范小结&quot;&gt;&lt;a href=&quot;#最常用命名规范小结&quot; class=&quot;headerlink&quot; title=&quot;最常用命名规范小结&quot;&gt;&lt;/a&gt;最常用命名规范小结&lt;/h4&gt;&lt;p&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/2019
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>越努力，越幸运</title>
    <link href="http://yoursite.com/2019/07/11/%E8%B6%8A%E5%8A%AA%E5%8A%9B%EF%BC%8C%E8%B6%8A%E5%B9%B8%E8%BF%90/"/>
    <id>http://yoursite.com/2019/07/11/越努力，越幸运/</id>
    <published>2019-07-11T10:48:33.616Z</published>
    <updated>2019-07-11T10:48:11.006Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>“凡是过往，皆为序章”</p></blockquote><p>刚结束本科四年的学习，迈向研究生的两个月暑假里，对未来有迷茫，有思考，有沮丧，但<strong>我是不会轻易被打倒的！</strong> 从现在开始，万事只能靠自己了，研究生阶段或许只能孤军奋战，但是我一定要坚持做出和本科一样的成绩，两年后找工作，简历一定要更加精彩！</p><blockquote><p>“一路向前，生如夏花”</p></blockquote><p>暑假两个月，老师没有安排，但是学习一定不能懈怠，我想养成一个学习的好习惯，每天坚持写作，可以是学习的内容，可以是我的心得体会，先知先觉，不要因为本硕连读就降低自己的期望值，不要害怕怀疑自己，<strong>加倍努力才能看起来毫不费力</strong>。我们总是会被周围环境影响，在一个散漫的氛围里，很容易堕落，要做就做实验室的一股清流，那就要出淤泥而不染，每时每刻都做好<strong>自我监督</strong>。当你投入其中一段时间，就会发现学术的乐趣，此时再没必要“自我监督”，而是真的用热情去做，这种“活在自己世界”的状态，是最容易出成果的。<br>在知乎上[ <a href="https://www.zhihu.com/question/64566768" target="_blank" rel="noopener">https://www.zhihu.com/question/64566768</a> ] 看到<strong>博士生在没有导师指导的情况下，该如何自己选题发 CVPR ？</strong> 话题，里面有很多方法可以学习，迷茫的时候可以看看。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;“凡是过往，皆为序章”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;刚结束本科四年的学习，迈向研究生的两个月暑假里，对未来有迷茫，有思考，有沮丧，但&lt;strong&gt;我是不会轻易被打倒的！&lt;/strong&gt; 从现在开始，万事只能靠自己了，研究生阶段或许
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://yoursite.com/2018/06/28/hello-world/"/>
    <id>http://yoursite.com/2018/06/28/hello-world/</id>
    <published>2018-06-28T03:13:14.147Z</published>
    <updated>2018-06-28T03:13:14.148Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
  </entry>
  
</feed>
